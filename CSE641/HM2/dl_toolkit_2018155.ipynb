{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "european-catholic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1613666005460,
     "user": {
      "displayName": "Lavanya Verma",
      "photoUrl": "",
      "userId": "04120516803627508822"
     },
     "user_tz": -330
    },
    "id": "european-catholic",
    "outputId": "1ffd6262-8e8a-4537-946d-dfc3b84fd33a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import scipy.special\n",
    "import sys\n",
    "import matplotlib.pyplot as  plt\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "QZG9cPlD2Ri5",
   "metadata": {
    "id": "QZG9cPlD2Ri5"
   },
   "outputs": [],
   "source": [
    "path = '/content/gdrive/My Drive/Colab Notebooks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vocal-winning",
   "metadata": {
    "executionInfo": {
     "elapsed": 1322,
     "status": "ok",
     "timestamp": 1613676441808,
     "user": {
      "displayName": "Lavanya Verma",
      "photoUrl": "",
      "userId": "04120516803627508822"
     },
     "user_tz": -330
    },
    "id": "vocal-winning"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "\n",
    "    acti_fns = ['relu', 'sigmoid', 'tanh']\n",
    "    weight_inits = ['random', 'he', 'xavier']\n",
    "    optimizers = ['gradient_descent','momentum', 'nag', 'adam', 'adagrad', 'rmsprop']\n",
    "    regularizations = ['l1', 'l2', 'layer_norm', 'batch_norm']\n",
    "    \n",
    "    def __init__(self, layers, num_epochs, dropouts, learning_rate = 1e-5, activation_function='relu', optimizer=\"gradient_descent\",\n",
    "weight_init=\"random\", regularization=\"l2\", batch_size=64, **kwargs):\n",
    "        \n",
    "        if (activation_function not in self.acti_fns):\n",
    "            raise Exception(\"Incorrect Weight Activation Function\")\n",
    "        if (optimizer not in self.optimizers):\n",
    "            raise Exception(\"Incorrect Optimizer\")\n",
    "        if (regularization not in self.regularizations):\n",
    "            raise Exception(\"Incorrect Regularizers\")\n",
    "        if (weight_init not in self.weight_inits):\n",
    "            raise Exception(\"Incorrect Weight Initialization\")\n",
    "        \n",
    "        np.random.seed(20)\n",
    "        \n",
    "        self.weights = []\n",
    "        self.weights_grad = []\n",
    "        self.inputs = []\n",
    "        for i in range(len(layers)-1):\n",
    "            if weight_init == \"random\":\n",
    "                w = np.random.random((layers[i]+1,layers[i+1])).astype(np.float128)*0.01\n",
    "                #else : he xavier\n",
    "            self.weights.append(w)\n",
    "            self.weights_grad.append(np.zeros(w.shape,dtype=np.float128))\n",
    "            \n",
    "\n",
    "    \n",
    "        if activation_function == 'relu':\n",
    "            self.acti_fns = self.relu\n",
    "            self.acti_fns_grad = self.relu_grad\n",
    "        elif activation_function == 'sigmoid':\n",
    "            self.acti_fns = self.sigmoid\n",
    "            self.acti_fns_grad = self.sigmoid_grad\n",
    "        elif activation_function == 'tanh':\n",
    "            self.acti_fns = self.tanh\n",
    "            self.acti_fns_grad = self.tanh_grad\n",
    "    \n",
    "    \n",
    "    \n",
    "        if optimizer == 'gradient_descent': \n",
    "            self.optimizer = self.gradient_descent\n",
    "            self.gradient_descent_init(learning_rate)\n",
    "        elif optimizer == 'momentum':\n",
    "            self.optimizer = self.momentum\n",
    "            self.momentum_init(learning_rate,beta= 0.9)\n",
    "        elif optimizer == 'nag':\n",
    "            self.optimizer = self.nag\n",
    "            self.nag_init()\n",
    "        elif optimizer == 'adam':\n",
    "            self.optimizer = self.adam\n",
    "            self.adam_init()\n",
    "        elif optimizer == 'adagrad':\n",
    "            self.optimizer = self.adagrad\n",
    "            self.adagrad_init()\n",
    "        elif optimizer =='rmsprop':\n",
    "            self.optimizer = self.rmsprop\n",
    "            self.rmsprop_init()\n",
    "    \n",
    "    \n",
    "    \n",
    "#         self.lr = learning_rate\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = num_epochs\n",
    "        self.dropouts = dropouts\n",
    "        self.regul = regularization\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        # make it loop over batches <---\n",
    "        training_loss = []\n",
    "        iterations = []\n",
    "        accuracy = []\n",
    "        for e in range(self.epochs):\n",
    "            rows = np.random.choice(X.shape[0],size = self.batch_size,replace = False)\n",
    "            x_batch = X[rows,:]\n",
    "            y_batch = Y[rows]\n",
    "            one_hot = np.eye(9 + 1)[y_batch] # D(s)\n",
    "            \n",
    "            probs = self.forward_pass(x_batch) # Y(s)           \n",
    "            predicted_labels = np.argmax(probs,axis = 1)\n",
    "            loss = self.loss(probs,one_hot)\n",
    "            acc = self.accuracy(predicted_labels,y_batch)           \n",
    "\n",
    "            print(\"---> Epoch: {}/{},Batch Size: {},Loss:{} ,Accuracy: {}\".format(e+1,self.epochs,self.batch_size,loss.sum(),acc))\n",
    "\n",
    "            training_loss.append(loss.sum())\n",
    "            iterations.append(e+1)\n",
    "            accuracy.append(acc)\n",
    "            \n",
    "            self.backward_pass(probs,one_hot)\n",
    "            self.optimizer()\n",
    "            \n",
    "            \n",
    "        self.iteration = iterations\n",
    "        self.training_loss = training_loss\n",
    "        self.training_accuracy = accuracy\n",
    "            \n",
    "    def forward_pass(self,X):\n",
    "        self.inputs =[] # Yi(s)\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "\n",
    "            X = np.append(X,np.ones((X.shape[0],1)),axis=1)\n",
    "            self.inputs.append(X)\n",
    "            X = np.matmul(X,self.weights[i])\n",
    "            if (i < len(self.weights) - 1):\n",
    "                X = self.acti_fns(X)\n",
    "            else :\n",
    "                X = self.softmax(X)\n",
    "        self.inputs.append(X)\n",
    "        return X\n",
    "        \n",
    "    def backward_pass(self,Y,D):\n",
    "        self.zero_grad()\n",
    "        dl_dy = self.loss_grad(Y,D)\n",
    "        for l in range(len(self.inputs)-1,0,-1):\n",
    "\n",
    "            if (l == len(self.inputs) -1 ):\n",
    "                # grad = self.softmax_grad(self.inputs[l])\n",
    "                dl_dz = Y - D\n",
    "            else: \n",
    "                grad = self.acti_fns_grad(self.inputs[l])\n",
    "                dl_dz = np.multiply(grad,dl_dy) \n",
    "\n",
    "            if (l == len(self.inputs) - 1):\n",
    "              self.weights_grad[l-1] += np.matmul(self.inputs[l-1].T,dl_dz)\n",
    "              dl_dy = np.matmul(dl_dz,self.weights[l-1].T)\n",
    "            else :\n",
    "              self.weights_grad[l-1] += np.matmul(self.inputs[l-1].T,dl_dz[:,:-1])\n",
    "              dl_dy = np.matmul(dl_dz[:,:-1],self.weights[l-1].T)\n",
    "            \n",
    "    def gradient_descent(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr*self.weights_grad[i]\n",
    "\n",
    "    def momentum(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.m[i] = self.beta*self.m[i] - self.lr*self.weights_grad[i]\n",
    "            self.weights[i] = self.weights[i]  + self.m[i]\n",
    "    \n",
    "    def nag(self):\n",
    "        pass\n",
    "    \n",
    "    def adam(self):\n",
    "        pass\n",
    "    \n",
    "    def adagrad(self):\n",
    "        pass\n",
    "    \n",
    "    def rmsprop(self):\n",
    "        pass\n",
    "    \n",
    "    def gradient_descent_init(self,learning_rate):\n",
    "        \"\"\" initialize learing rate\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        pass\n",
    "    \n",
    "    def momentum_init(self,learning_rate,beta):\n",
    "        self.beta = beta\n",
    "        self.m = []\n",
    "        for i in range(len(self.weights)):\n",
    "            self.m.append(np.zeros(self.weights[i].shape))\n",
    "        self.lr = learning_rate \n",
    "    \n",
    "    def nag_init(self):\n",
    "        pass\n",
    "    \n",
    "    def adam_init(self):\n",
    "        pass\n",
    "    \n",
    "    def adagrad_init(self):\n",
    "        pass\n",
    "    \n",
    "    def rmsprop_init(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self,X):  \n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs,axis=1)\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        return self.forward_pass(X)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.weights\n",
    "        \n",
    "    def score(self,X,Y):\n",
    "        pass \n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for i in range(len(self.weights_grad)):\n",
    "            self.weights_grad[i] = np.zeros(self.weights_grad[i].shape)\n",
    "\n",
    "    def loss(self,Y,D):\n",
    "        ''' Cross_Entropy Loss\n",
    "            Y: probs (n_instances,M)\n",
    "            D: Actual Labels (n_instances,M); M:number of neurons in the last layer\n",
    "        '''\n",
    "        noise = 1e-9\n",
    "        return -1*np.mean(D*np.log(Y+noise))\n",
    "    \n",
    "    def loss_grad(self,Y,D):\n",
    "        '''Y : probabilites (n_instances,M)\n",
    "           D : Actual Labels(n_instances,M); categorical cross entropy (not used)\n",
    "        '''\n",
    "        N = Y + 1e-9\n",
    "        N = -1/N\n",
    "        return N\n",
    "    \n",
    "    \n",
    "    def relu(self,X):\n",
    "        r = X.copy()\n",
    "        r[X<0] = 0\n",
    "        return r\n",
    "\n",
    "    def relu_grad(self,X):\n",
    "        r = X.copy()\n",
    "        r[X>=0] = 1\n",
    "        r[X<0] = 0\n",
    "        return r\n",
    "    \n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def tanh_grad(self,X):\n",
    "        return 1 - self.tanh(X)**2\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1 + np.exp(-X))\n",
    "    \n",
    "    def sigmoid_grad(self,X):\n",
    "        a = self.sigmoid(X)\n",
    "        return a*(1 - a)\n",
    "    \n",
    "    def softmax(self,X):\n",
    "        exp = (X - X.mean(axis= 1).reshape((X.shape[0],1)))\n",
    "        # X = np.exp(X)/np.sum(np.exp(X),axis= 1)[:,None] \n",
    "        exp = scipy.special.softmax(exp ,axis=1)\n",
    "        return exp\n",
    "                \n",
    "    def softmax_grad(self,X):\n",
    "        ''' there is an other of derivative, however for gradient computation this term suffices'''\n",
    "        S = self.softmax(X)\n",
    "        out = S*(1- S)\n",
    "        return out\n",
    "    \n",
    "    def accuracy(self,predicted_labels,true_labels):\n",
    "        s = predicted_labels == true_labels\n",
    "        return s.sum()/s.shape[0]\n",
    "        \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "limited-lobby",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 40866,
     "status": "ok",
     "timestamp": 1613679354512,
     "user": {
      "displayName": "Lavanya Verma",
      "photoUrl": "",
      "userId": "04120516803627508822"
     },
     "user_tz": -330
    },
    "id": "limited-lobby",
    "outputId": "d6947979-b018-4458-fdf3-204b6d266095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch: 1/500,Batch Size: 64,Loss:0.2301017779517305 ,Accuracy: 0.0625\n",
      "---> Epoch: 2/500,Batch Size: 64,Loss:1.2272503069494434 ,Accuracy: 0.140625\n",
      "---> Epoch: 3/500,Batch Size: 64,Loss:0.24498258349864976 ,Accuracy: 0.140625\n",
      "---> Epoch: 4/500,Batch Size: 64,Loss:0.31673391490966546 ,Accuracy: 0.109375\n",
      "---> Epoch: 5/500,Batch Size: 64,Loss:0.2776804690059273 ,Accuracy: 0.109375\n",
      "---> Epoch: 6/500,Batch Size: 64,Loss:0.24649649506746182 ,Accuracy: 0.0625\n",
      "---> Epoch: 7/500,Batch Size: 64,Loss:0.253835423108267 ,Accuracy: 0.03125\n",
      "---> Epoch: 8/500,Batch Size: 64,Loss:0.23663220634418772 ,Accuracy: 0.09375\n",
      "---> Epoch: 9/500,Batch Size: 64,Loss:0.23377823852092433 ,Accuracy: 0.140625\n",
      "---> Epoch: 10/500,Batch Size: 64,Loss:0.23072455786801518 ,Accuracy: 0.15625\n",
      "---> Epoch: 11/500,Batch Size: 64,Loss:0.2324358177545772 ,Accuracy: 0.046875\n",
      "---> Epoch: 12/500,Batch Size: 64,Loss:0.23614141522552476 ,Accuracy: 0.109375\n",
      "---> Epoch: 13/500,Batch Size: 64,Loss:0.22993263425022853 ,Accuracy: 0.140625\n",
      "---> Epoch: 14/500,Batch Size: 64,Loss:0.2375184438127303 ,Accuracy: 0.09375\n",
      "---> Epoch: 15/500,Batch Size: 64,Loss:0.23235059201287248 ,Accuracy: 0.140625\n",
      "---> Epoch: 16/500,Batch Size: 64,Loss:0.23115836713446253 ,Accuracy: 0.140625\n",
      "---> Epoch: 17/500,Batch Size: 64,Loss:0.2356020713697853 ,Accuracy: 0.078125\n",
      "---> Epoch: 18/500,Batch Size: 64,Loss:0.22763665571683536 ,Accuracy: 0.171875\n",
      "---> Epoch: 19/500,Batch Size: 64,Loss:0.23417526114236353 ,Accuracy: 0.140625\n",
      "---> Epoch: 20/500,Batch Size: 64,Loss:0.23722047300948113 ,Accuracy: 0.078125\n",
      "---> Epoch: 21/500,Batch Size: 64,Loss:0.22689129645604103 ,Accuracy: 0.140625\n",
      "---> Epoch: 22/500,Batch Size: 64,Loss:0.24220122246986925 ,Accuracy: 0.046875\n",
      "---> Epoch: 23/500,Batch Size: 64,Loss:0.293940679196962 ,Accuracy: 0.078125\n",
      "---> Epoch: 24/500,Batch Size: 64,Loss:0.3160993549542907 ,Accuracy: 0.140625\n",
      "---> Epoch: 25/500,Batch Size: 64,Loss:0.26305243169533027 ,Accuracy: 0.03125\n",
      "---> Epoch: 26/500,Batch Size: 64,Loss:0.2634842531950351 ,Accuracy: 0.234375\n",
      "---> Epoch: 27/500,Batch Size: 64,Loss:0.3175024945156083 ,Accuracy: 0.09375\n",
      "---> Epoch: 28/500,Batch Size: 64,Loss:0.23474912399314588 ,Accuracy: 0.078125\n",
      "---> Epoch: 29/500,Batch Size: 64,Loss:0.3877897543808494 ,Accuracy: 0.15625\n",
      "---> Epoch: 30/500,Batch Size: 64,Loss:0.23404430587851147 ,Accuracy: 0.125\n",
      "---> Epoch: 31/500,Batch Size: 64,Loss:0.23591944837909268 ,Accuracy: 0.109375\n",
      "---> Epoch: 32/500,Batch Size: 64,Loss:0.20792194782003748 ,Accuracy: 0.21875\n",
      "---> Epoch: 33/500,Batch Size: 64,Loss:0.22847931414054223 ,Accuracy: 0.125\n",
      "---> Epoch: 34/500,Batch Size: 64,Loss:0.22419609617336644 ,Accuracy: 0.15625\n",
      "---> Epoch: 35/500,Batch Size: 64,Loss:0.2876496621100944 ,Accuracy: 0.203125\n",
      "---> Epoch: 36/500,Batch Size: 64,Loss:0.2693298965479226 ,Accuracy: 0.234375\n",
      "---> Epoch: 37/500,Batch Size: 64,Loss:0.30622123154175046 ,Accuracy: 0.109375\n",
      "---> Epoch: 38/500,Batch Size: 64,Loss:0.2804909202209261 ,Accuracy: 0.171875\n",
      "---> Epoch: 39/500,Batch Size: 64,Loss:0.44269553276067986 ,Accuracy: 0.15625\n",
      "---> Epoch: 40/500,Batch Size: 64,Loss:0.6973706330504634 ,Accuracy: 0.171875\n",
      "---> Epoch: 41/500,Batch Size: 64,Loss:0.7361673567835855 ,Accuracy: 0.15625\n",
      "---> Epoch: 42/500,Batch Size: 64,Loss:0.4142839525202957 ,Accuracy: 0.0625\n",
      "---> Epoch: 43/500,Batch Size: 64,Loss:0.8259788398376005 ,Accuracy: 0.203125\n",
      "---> Epoch: 44/500,Batch Size: 64,Loss:0.26903674130678906 ,Accuracy: 0.125\n",
      "---> Epoch: 45/500,Batch Size: 64,Loss:0.2501225663287316 ,Accuracy: 0.234375\n",
      "---> Epoch: 46/500,Batch Size: 64,Loss:0.5684499558099371 ,Accuracy: 0.140625\n",
      "---> Epoch: 47/500,Batch Size: 64,Loss:1.3116896049038422 ,Accuracy: 0.21875\n",
      "---> Epoch: 48/500,Batch Size: 64,Loss:1.766903703734671 ,Accuracy: 0.125\n",
      "---> Epoch: 49/500,Batch Size: 64,Loss:0.8849199409345542 ,Accuracy: 0.109375\n",
      "---> Epoch: 50/500,Batch Size: 64,Loss:0.39319903770839726 ,Accuracy: 0.09375\n",
      "---> Epoch: 51/500,Batch Size: 64,Loss:0.2874607419502229 ,Accuracy: 0.140625\n",
      "---> Epoch: 52/500,Batch Size: 64,Loss:0.2580109144514346 ,Accuracy: 0.125\n",
      "---> Epoch: 53/500,Batch Size: 64,Loss:0.33617565129787214 ,Accuracy: 0.234375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cf86525aaf9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m n = MLPClassifier(layers = [784,128,10], dropouts = 0.2,num_epochs=500, learning_rate = 1e-1, activation_function='relu', optimizer=\"gradient_descent\",\n\u001b[1;32m      2\u001b[0m weight_init=\"random\",batch_size=64)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f9796a7fee0f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f9796a7fee0f>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, Y, D)\u001b[0m\n\u001b[1;32m    134\u001b[0m               \u001b[0mdl_dy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_dz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl_dz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m               \u001b[0mdl_dy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_dz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = MLPClassifier(layers = [784,128,10], dropouts = 0.2,num_epochs=500, learning_rate = 1e-1, activation_function='relu', optimizer=\"gradient_descent\",\n",
    "weight_init=\"random\",batch_size=64)\n",
    "n.fit(x_train,y_train)\n",
    "plt.plot(n.iteration,n.training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beneficial-coach",
   "metadata": {
    "id": "beneficial-coach"
   },
   "outputs": [],
   "source": [
    "def upload_datasets():\n",
    "    val_set = joblib.load('val_set.pkl')\n",
    "    train_set = joblib.load('train_set.pkl')\n",
    "    return train_set,val_set\n",
    "def preprocess_data(dataset):\n",
    "    x_temp = dataset['Image'].values\n",
    "    x = []\n",
    "    y = dataset['Labels'].values\n",
    "#     y = np.array(y).reshape((y.shape[0],1))\n",
    "    for i in range(0,len(x_temp)):\n",
    "        x.append(np.array(x_temp[i]).reshape(784,)/255)\n",
    "    x = np.array(x)\n",
    "    return x,y\n",
    "train_set, val_set = upload_datasets()\n",
    "x_train, y_train = preprocess_data(train_set)\n",
    "x_val , y_val = preprocess_data(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-sister",
   "metadata": {
    "id": "laden-sister",
    "outputId": "5e66a1b9-4b11-4957-bc97-421203aa2342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 291,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(y_val.max()+1)[y_val]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dl_toolkit_2018155.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
