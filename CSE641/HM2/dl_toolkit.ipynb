{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "european-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import scipy.special\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "vocal-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "\n",
    "    acti_fns = ['relu', 'sigmoid', 'tanh']\n",
    "    weight_inits = ['random', 'he', 'xavier']\n",
    "    optimizers = ['gradient_descent','momentum', 'nag', 'adam', 'adagrad', 'rmsprop']\n",
    "    regularizations = ['l1', 'l2', 'layer_norm', 'batch_norm']\n",
    "    \n",
    "    def __init__(self, layers, num_epochs, dropouts, learning_rate = 1e-5, activation_function='relu', optimizer=\"gradient_descent\",\n",
    "weight_init=\"random\", regularization=\"l2\", batch_size=64, **kwargs):\n",
    "        \n",
    "        if (activation_function not in self.acti_fns):\n",
    "            raise Exception(\"Incorrect Weight Activation Function\")\n",
    "        if (optimizer not in self.optimizers):\n",
    "            raise Exception(\"Incorrect Optimizer\")\n",
    "        if (regularization not in self.regularizations):\n",
    "            raise Exception(\"Incorrect Regularizers\")\n",
    "        if (weight_init not in self.weight_inits):\n",
    "            raise Exception(\"Incorrect Weight Initialization\")\n",
    "        \n",
    "        \n",
    "        self.weights = []\n",
    "        self.weights_grad = []\n",
    "        self.inputs = []\n",
    "        for i in range(len(layers)-1):\n",
    "            if weight_init == \"random\":\n",
    "                w = np.random.random((layers[i]+1,layers[i+1])).astype(np.float128)\n",
    "                #else : he xavier\n",
    "            self.weights.append(w)\n",
    "            self.weights_grad.append(np.zeros(w.shape,dtype=np.float128))\n",
    "            \n",
    "\n",
    "    \n",
    "        if activation_function == 'relu':\n",
    "            self.acti_fns = self.relu\n",
    "            self.acti_fns_grad = self.relu_grad\n",
    "        elif activation_function == 'sigmoid':\n",
    "            self.acti_fns = self.sigmoid\n",
    "            self.acti_fns_grad = self.sigmoid_grad\n",
    "        elif activation_function == 'tanh':\n",
    "            self.acti_fns = self.tanh\n",
    "            self.acti_fns_grad = self.tanh_grad\n",
    "    \n",
    "    \n",
    "    \n",
    "        if optimizer == 'gradient_descent': \n",
    "            self.optimizer = self.gradient_descent\n",
    "            self.gradient_descent_init(learning_rate)\n",
    "        elif optimizer == 'momentum':\n",
    "            self.optimizer = self.momentum\n",
    "            self.momentum_init()\n",
    "        elif optimizer == 'nag':\n",
    "            self.optimizer = self.nag\n",
    "            self.nag_init()\n",
    "        elif optimizer == 'adam':\n",
    "            self.optimizer = self.adam\n",
    "            self.adam_init()\n",
    "        elif optimizer == 'adagrad':\n",
    "            self.optimizer = self.adagrad\n",
    "            self.adagrad_init()\n",
    "        elif optimizer =='rmsprop':\n",
    "            self.optimizer = self.rmsprop\n",
    "            self.rmsprop_init()\n",
    "    \n",
    "    \n",
    "    \n",
    "#         self.lr = learning_rate\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = num_epochs\n",
    "        self.dropouts = dropouts\n",
    "        self.regul = regularization\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        # make it loop over batches <---\n",
    "        training_loss = []\n",
    "        iterations = []\n",
    "        for e in range(self.epochs):\n",
    "            rows = np.random.choice(X.shape[0],size = self.batch_size,replace = False)\n",
    "            x_batch = X[rows,:]\n",
    "            y_batch = Y[rows]\n",
    "            one_hot = np.eye(y_batch.max() + 1)[y_batch] # D(s)\n",
    "            \n",
    "            \n",
    "            probs = self.forward_pass(x_batch) # Y(s)           \n",
    "            predicted_labels = np.argmax(probs,axis = 1)\n",
    "            loss = self.loss(probs,one_hot)\n",
    "            acc = self.accuracy(predicted_labels,y_batch)\n",
    "#             print(probs,loss,acc)\n",
    "#             sys.exit()\n",
    "#             print(\"probs:{},pred:{},y_batch:{},one_hot:{}\".format(probs.shape,predicted_labels.shape,y_batch.shape,one_hot.shape))\n",
    "            \n",
    "            \n",
    "            self.backward_pass(probs,one_hot)\n",
    "            self.optimizer()\n",
    "            \n",
    "            print(\"---> Epoch: {}/{},Batch Size: {},Loss:{} ,Accuracy: {}\".format(e+1,epochs,self.batch_size,loss,acc))\n",
    "            training_loss.append(loss)\n",
    "            iterations.append(e+1)\n",
    "            \n",
    "    def forward_pass(self,X):\n",
    "        self.inputs =[] # Yi(s)\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            X = np.append(X,np.ones((X.shape[0],1)),axis=1)\n",
    "            self.inputs.append(X)\n",
    "            X = np.matmul(X,self.weights[i])\n",
    "            if (i < len(self.weights) - 1):\n",
    "                X = self.acti_fns(X)\n",
    "            else :\n",
    "                X = self.softmax(X)\n",
    "        self.inputs.append(X)\n",
    "        return X\n",
    "        \n",
    "    def backward_pass(self,Y,D):\n",
    "        self.zero_grad()\n",
    "        dl_dy = self.loss_grad(Y,D)\n",
    "        for l in range(len(self.inputs)-1,1,-1):\n",
    "            if (l == len(self.inputs) -1 ):\n",
    "                grad = self.softmax_grad(self.inputs[l])\n",
    "            else: \n",
    "                grad = self.acti_fns_grad(self.inputs[l])\n",
    "            dl_dz = np.multiply(grad,dl_dy)\n",
    "            for instance in range(dl_dz.shape[0]):\n",
    "                for i in range(self.weights_grad[l-1].shape[0]):\n",
    "                    for j in range(self.weights_grad[l-1].shape[1]):\n",
    "                        self.weights_grad[l-1][i,j] += dl_dz[instance,j]*self.inputs[l-1][instance,i]\n",
    "            dl_dy = np.matmul(dl_dz,self.weights[l-1].T)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.weights_grad[i]\n",
    "        \n",
    "            \n",
    "    \n",
    "    def momentum(self):\n",
    "        pass\n",
    "    \n",
    "    def nag(self):\n",
    "        pass\n",
    "    \n",
    "    def adam(self):\n",
    "        pass\n",
    "    \n",
    "    def adagrad(self):\n",
    "        pass\n",
    "    \n",
    "    def rmsprop(self):\n",
    "        pass\n",
    "    \n",
    "    def gradient_descent_init(self,learning_rate):\n",
    "        \"\"\" initialize learing rate\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        pass\n",
    "    \n",
    "    def momentum_init(self):\n",
    "        pass\n",
    "    \n",
    "    def nag_init(self):\n",
    "        pass\n",
    "    \n",
    "    def adam_init(self):\n",
    "        pass\n",
    "    \n",
    "    def adagrad_init(self):\n",
    "        pass\n",
    "    \n",
    "    def rmsprop_init(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self,X):  \n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs,axis=1)\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        return self.forward_pass(X)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.weights\n",
    "        \n",
    "    def score(self,X,Y):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(len(self.weights_grad)):\n",
    "            self.weights_grad[i] = np.zeros(self.weights_grad[i].shape)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loss(self,Y,D):\n",
    "        ''' Y: probs (n_instances,M)\n",
    "            D: Actual Labels (n_instances,M); M:number of neurons in the last layer\n",
    "        '''\n",
    "        return np.multiply(np.log(Y + 1e-9),-D).sum(axis=1)\n",
    "    \n",
    "    def loss_grad(self,Y,D):\n",
    "        '''Y : probabilites (n_instances,M)\n",
    "           D : Actual Labels(n_instances,M); categorical cross entropy\n",
    "        '''\n",
    "        N = Y.copy() + 1e-9\n",
    "        N = -1/N\n",
    "        return N\n",
    "    \n",
    "    \n",
    "    def relu(self,X):\n",
    "        X[X<0] = 0\n",
    "        return X\n",
    "\n",
    "    def relu_grad(self,X):\n",
    "        X[X>=0] = 1\n",
    "        X[X<0] = 0\n",
    "        return X\n",
    "    \n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def tanh_grad(self,X):\n",
    "        return 1 - self.tanh(X)**2\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1 + np.exp(-X))\n",
    "    \n",
    "    def sigmoid_grad(self,X):\n",
    "        a = self.sigmoid(X)\n",
    "        return a*(1 - a)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def softmax(self,X):\n",
    "#         X = X.astype(np.float128)\n",
    "#         X = np.exp(X)/np.sum(np.exp(X),axis= 1)[:,None]\n",
    "        X = scipy.special.softmax(X,axis=1)\n",
    "        return X\n",
    "        \n",
    "        \n",
    "    def softmax_grad(self,X):\n",
    "        ''' there is an other of derivative, however for gradient computation this term suffices\n",
    "        '''\n",
    "        S = self.softmax(X)\n",
    "        out = S*(1- S)\n",
    "        return out\n",
    "    \n",
    "    def accuracy(self,predicted_labels,true_labels):\n",
    "        s = predicted_labels == true_labels\n",
    "        return s.sum()/s.shape[0]\n",
    "        \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "limited-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLPClassifier(layers = [784,512,256,10], dropouts = 0.2,num_epochs=1, learning_rate = 1e-5, activation_function='relu', optimizer=\"gradient_descent\",\n",
    "weight_init=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "educated-corpus",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 256 is different from 257)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-1de589045802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-c80f42e43da4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-c80f42e43da4>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, Y, D)\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdl_dz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mdl_dy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_dz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 256 is different from 257)"
     ]
    }
   ],
   "source": [
    "n.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "pending-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.matmul(n.inputs[2],n.weights[2])\n",
    "r = r.astype(np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-cheat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "dominant-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float128)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(r,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beneficial-coach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Image  Labels\n",
       "0     <PIL.Image.Image image mode=L size=28x28 at 0x...       5\n",
       "1     <PIL.Image.Image image mode=L size=28x28 at 0x...       0\n",
       "2     <PIL.Image.Image image mode=L size=28x28 at 0x...       4\n",
       "3     <PIL.Image.Image image mode=L size=28x28 at 0x...       1\n",
       "4     <PIL.Image.Image image mode=L size=28x28 at 0x...       9\n",
       "...                                                 ...     ...\n",
       "9995  <PIL.Image.Image image mode=L size=28x28 at 0x...       5\n",
       "9996  <PIL.Image.Image image mode=L size=28x28 at 0x...       5\n",
       "9997  <PIL.Image.Image image mode=L size=28x28 at 0x...       5\n",
       "9998  <PIL.Image.Image image mode=L size=28x28 at 0x...       5\n",
       "9999  <PIL.Image.Image image mode=L size=28x28 at 0x...       5\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def upload_datasets():\n",
    "    val_set = joblib.load('val_set.pkl')\n",
    "    train_set = joblib.load('train_set.pkl')\n",
    "    return train_set,val_set\n",
    "def preprocess_data(dataset):\n",
    "    x_temp = dataset['Image'].values\n",
    "    x = []\n",
    "    y = dataset['Labels'].values\n",
    "#     y = np.array(y).reshape((y.shape[0],1))\n",
    "    for i in range(0,len(x_temp)):\n",
    "        x.append(np.array(x_temp[i]).reshape(784,)/255)\n",
    "    x = np.array(x)\n",
    "    return x,y\n",
    "train_set, val_set = upload_datasets()\n",
    "x_train, y_train = preprocess_data(train_set)\n",
    "x_val , y_val = preprocess_data(val_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "laden-sister",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(y_val.max()+1)[y_val]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
